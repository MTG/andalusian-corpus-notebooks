{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arab-Andalusian Corpus - Nawba Recognition using Templates from Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook computes several experiments to evaluate the performance of templates to recognize the nawba of a Arab-Andalusian recording. Each template is synthesized from several folded pitch class distributions belonging to a nawba using Gaussian distribution. The folded pitch distribution of a track is compared to the templates and the best match predicts the nawba. \n",
    "The experiments test different distance measures and standard deviation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizialization (MANDATORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, all the libraries are loaded. \n",
    "Furthermore, a function check if the metadata related to the Arab-Andalusian corpus of Dunya has been downloaded. If necessary, all the metadata will be downloaded. \n",
    "At the end, an object to manage the Dunya metadata is created.\n",
    "\n",
    "#### NB: Before to run, remember to add the dunya token in the costants.py file. This file is in the directory \"utilities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "\n",
    "from shutil import copyfile\n",
    "from utilities.recordingcomputation import *\n",
    "from utilities.dunyautilities import *\n",
    "from utilities.metadataStatistics import *\n",
    "from utilities.generalutilities import *\n",
    "from utilities.experiments import *\n",
    "\n",
    "# download metadata from Dunya\n",
    "if not check_dunya_metadata():\n",
    "    print(\"Downloading metadata from Dunya...\")\n",
    "    collect_metadata()\n",
    "\n",
    "# create an object with all the well-structured metadata\n",
    "print(\"Analyzing Dunya Metadata...\")\n",
    "cm = CollectionMetadata()\n",
    "print(\"Collection of metadata created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation (MANDATORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty object to manage the dataset of the experiments is created.\n",
    "Than, a list of recordings are imported from a csv and added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty object\n",
    "do = DataSet(cm)\n",
    "csv_filename = \"dataset_nawba_77_recordings.csv\" #\"dataset_test.csv\" \"dataset_nawba_77_recordings.csv\"\n",
    "# add recording mbids of an external file in the dataset \n",
    "do.import_dataset_from_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nawba Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the experiments are defined. An object is created for every experiment and added to a list.\n",
    "\n",
    "The distance measure parameters are: \"city block (L1)\", \"euclidean (L2)\", \"correlation\", \"canberra\".\n",
    "\n",
    "The standard deviation values tested are 20, 30 and 40, but they could be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_measures_list = [\"euclidean (L2)\"] #[\"city block (L1)\", \"euclidean (L2)\", \"correlation\", \"canberra\"]\n",
    "random_state = 20\n",
    "std_list = [30] #[20,30,40]\n",
    "esperiment_name = \"nr_L2_20_30_with_correct_nawba\"\n",
    "source_dir = os.path.join(EXPERIMENT_DIR, esperiment_name)\n",
    "sub_esperiment_suffix = \"exp\"\n",
    "\n",
    "experiment_list = list()\n",
    "for i_element in range(7):  \n",
    "    sub_esperiment_name = \"{}_{}\".format(sub_esperiment_suffix, i_element + 1)\n",
    "    experiment_list.append(Nawba_Recognition_Experiment(do, i_element, random_state, std_list, distance_measures_list, sub_esperiment_name, source_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, the experiments will be computed. If the plot_flag is True the plots of the templates and of the best matches will be saved in the experiment directory as png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "plot_flag = True\n",
    "zip_path = \"dataset_nawba_77_recordings.zip\"\n",
    "\n",
    "# check if all the necessary files related to each recording of the dataset are available. \n",
    "recordings_with_missing_files = experiment_list[0].get_recordings_without_experiment_files()\n",
    "# If not import the file from zip\n",
    "if len(recordings_with_missing_files) != 0:\n",
    "    extract_files_from_zip(RECORDINGS_DIR, zip_path)\n",
    "    # second check after unzip\n",
    "    recordings_with_missing_files = experiment_list[0].get_recordings_without_experiment_files()\n",
    "    if len(recordings_with_missing_files) != 0:\n",
    "        raise Exception (\"A/some file/s is/are missing\")\n",
    "\n",
    "# run the experiment\n",
    "for index in range(len(experiment_list)):\n",
    "    name = \"exp_{} results: \".format(counter)\n",
    "    print()\n",
    "    print(name)\n",
    "    experiment_list[index].run(plot_flag=plot_flag)\n",
    "    experiment_list[index].compute_summary()\n",
    "    print()\n",
    "    print(experiment_list[index].df_summary )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall results will be computed and all the results of each experiment will be exported in csv and stored the experiment directory. The confusion matrix of the best parameters combination will be plotted in a png file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print overall results\n",
    "df_overall =  experiment_list[0].df_summary\n",
    "for index in range(len(experiment_list)-1):\n",
    "    df_overall = df_overall.add(experiment_list[index+1].df_summary)\n",
    "df_overall = df_overall.divide(len(experiment_list))\n",
    "print(df_overall)\n",
    "\n",
    "# export the results\n",
    "export_overall_experiment(experiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export in json the dataset in order to use it in the machine learning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmbid_list = do.get_rmbid_list()\n",
    "list_of_dicts = list()\n",
    "\n",
    "for rmbid in rmbid_list:\n",
    "\n",
    "    df_sections = cm.get_description_of_single_recording(rmbid)\n",
    "    column_list = [\"tab\", \"tonic\", \"start_time\", \"end_time\"]\n",
    "\n",
    "    section_dict = list()\n",
    "    for row_index in range(len(df_sections.index.values.tolist())):\n",
    "        row_dict = dict()\n",
    "        row_dict[column_list[0]] = df_sections.loc[row_index, column_list[0]] \n",
    "        row_dict[column_list[2]] = df_sections.loc[row_index, column_list[2]] \n",
    "        row_dict[column_list[3]] = df_sections.loc[row_index, column_list[3]]\n",
    "        row_dict[column_list[1]] = random.uniform(100, 200) # TODO: add the real value\n",
    "        section_dict.append(row_dict)\n",
    "\n",
    "    #df_reduced = pd.DataFrame(0, columns = column_list, index = df_sections.index.values.tolist())\n",
    "    #for row_index in range(len(df_sections.index.values.tolist())):\n",
    "    #    df_reduced.loc[row_index, column_list[0]] = df_sections.loc[row_index, column_list[0]] \n",
    "    #    df_reduced.loc[row_index, column_list[2]] = df_sections.loc[row_index, column_list[2]] \n",
    "    #    df_reduced.loc[row_index, column_list[3]] = df_sections.loc[row_index, column_list[3]] \n",
    "        # TODO: load real value\n",
    "    #    df_reduced.loc[row_index, column_list[3]] = random.uniform(100, 200)\n",
    "\n",
    "    main_dict = dict()\n",
    "    main_dict[\"mbid\"] = rmbid\n",
    "    main_dict[\"nawba\"] = df_sections.loc[0,\"nawba\"]\n",
    "    main_dict[\"section\"] = section_dict\n",
    "    #print(main_dict[\"section\"])\n",
    "    list_of_dicts.append(main_dict)\n",
    "\n",
    "json_path = os.path.join(DATA_DIR, \"dataset_77_tab_tonic.json\")\n",
    "with open(json_path, 'w') as outfile:\n",
    "    json.dump(list_of_dicts, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract dataset pitch distributions and xmls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmbid_list = do.get_rmbid_list()\n",
    "dir_name = \"dataset\"\n",
    "main_path = os.path.join(DATA_DIR, dir_name)\n",
    "for rmbid in rmbid_list:\n",
    "    # create a directory for all rmbid\n",
    "    rmbid_dir = os.path.join(main_path, rmbid)\n",
    "    if not os.path.exists(rmbid_dir):\n",
    "        os.makedirs(rmbid_dir)\n",
    "    input_dir = os.path.join(RECORDINGS_DIR, rmbid)\n",
    "    input_pd = os.path.join(input_dir, FN_PD)\n",
    "    score_name = \"{}.xml\".format(rmbid)\n",
    "    input_score = os.path.join(input_dir, score_name)\n",
    "    output_pd = os.path.join(rmbid_dir, FN_PD)\n",
    "    output_score = os.path.join(rmbid_dir, score_name)\n",
    "    \n",
    "    copyfile(input_pd, output_pd)\n",
    "    copyfile(input_score, output_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
